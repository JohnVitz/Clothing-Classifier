{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM700a7R13L+n7AxsIaVDrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnVitz/Clothing-Classifier/blob/main/Clothing_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quick Start Tutorial**\n",
        "\n",
        "Reference: https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
        "\n",
        "Basic model for predicting clothes"
      ],
      "metadata": {
        "id": "OOeRKba4-St7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SVmutt9l6TVS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## here we create our training data\n",
        "training_data = datasets.FashionMNIST(\n",
        "      root=\"data\", # where to store the data (file location)\n",
        "      train=True, #training vs test data\n",
        "      download=True, #download dataset to root place\n",
        "      transform=ToTensor(), #transform to tensor info\n",
        ")\n",
        "\n",
        "## here we create our test data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # this gives us test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV7gFqrf7Rex",
        "outputId": "8a21314e-0f1e-440e-e54e-8427a552dbd3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.8MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 305kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.16MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we need to pass the dataset to the data loader\n",
        "## this supports auotmatic batching, sampling and shuffling\n",
        "\n",
        "batch_size = 64 #each element in the dataloader will return batch of 64 features\n",
        "\n",
        "# create data loaders\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "  #wrap dataset into an iteratable\n",
        "  #batches the data and shuffles it\n",
        "  #x.shape = [64,1,28,28] --> 64 images, grayscale, each 28x28)\n",
        "  #y.shape = [64] --> 64 labels\n",
        "\n",
        "for X, y in test_dataloader: ## this iterates through the batch\n",
        "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    # x = batch of image tensors in the shape [N, C, H, W]\n",
        "  print(f\"Shape of y: {y.shape}{y.dtype}\")\n",
        "    # y = batch of integer labels (e.g. 0 = Tshirt)\n",
        "  break ## this stops the loop once which gives you a peek at your data set\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSzekYsk8H-c",
        "outputId": "ad8b0c99-12ec-490b-d885-79769bea29a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64])torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creating our model\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "## define our model\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,10)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KV1i-mF-n9G",
        "outputId": "dfbb86d4-03be-4a39-91b5-81d90680c656"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Our Model\n",
        "\n",
        "this is the full code block\n",
        "```\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "```\n",
        "\n",
        "## define our model\n",
        "```\n",
        "class NeuralNetwork(nn.Module):\n",
        "```\n",
        "nn.module is blueprint for all neural networks. By defining our class with this, it means we are extending this model rather than starting it from scratch\n",
        "\n",
        "```\n",
        "  def __init__(self):\n",
        "```  \n",
        "This is the constructor for your model. It defines the layers/strcuture.\n",
        "In this code, we are saying that we are going to start defining the layers for this model.\n",
        "\n",
        "```\n",
        "super().__init__()\n",
        "```\n",
        "This calls the parent class constructor and make sure that your layers have all of the standard thigns\n",
        "\n",
        "together, this is akin to saying\n",
        "def __init__(self) --> I am opening a new franchise restaurant and want it to contain x, y, z features\n",
        "super().__init__() --> I also want to make sure all the standard stuff is included to like a cash register, logo\n",
        "\n",
        "```\n",
        "self.flatten = nn.Flatten()\n",
        "```\n",
        "This is telling us to add this layer to the model and remember it. This is ultimately used as a layer to flatten.\n",
        "\n",
        "Why do we do this separately? We could have added it to the layer sequence below but didnt because:\n",
        "* nn.Sequential only works when each layer has the same input/output pattern. It mostly expects 2D\n",
        "* This also makes it clear that its a pre-processing step\n",
        "* It makes it easier to adjust in the future because you can just change how you flatten the image\n",
        "\n",
        "```\n",
        "self.linear_relu_stack = nn.Sequential(\n",
        "```\n",
        "A layer is ultimately just a processing step in a neural network. We pass something through and the layer transforms it some way.\n",
        "\n",
        "--> self.linear_relu_stack = I am creating a bundle of layers (mini pipeline) and I'm naming it this linear_relu_stack so i can refer to later\n",
        "*   Self means store it in the model for future reference\n",
        "*.  linear_relu_stack is just a name that you can name anything else\n",
        "\n",
        "--> Sequential means stack the layers and apply them one after another like a pipeline\n",
        "\n",
        "nn.Linear = fully connected dense layer\n",
        "\n",
        "*   nn.Linear = fully connect dense layer\n",
        "*   nn.ReLU = activation function\n",
        "*   nn.Conv2D = convolutional layer\n",
        "*  mm.Flatten = multi-dim vector into a flat vector\n",
        "\n",
        "## now lets break down the layers\n",
        "\n",
        "```\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "\n",
        "```\n",
        "```nn.Linear(28*28, 512)```\n",
        "\n",
        "This is a fully connected layer (aka Dense Layer).\n",
        "It takes the 784-dimension output (28*28) and outputs a 512 dimensional vector. This smaller and more meaningful for the model to learn from\n",
        "*   512 is more meaningful here because we are getting rid of the noise and summarizing the important parts\n",
        "*  512 is arbitrary but common choice. Its a ```hyperparameter``` and chosen through trila, convetion, or habit\n",
        "* 512 is a natural default when you want a big-ish layer. It gives enough to learn, but not slowing it down too much.\n",
        "* This is a place where you could tune it yourself and see what the outputs look like. You can do hyper-parameter tuning\n",
        "\n",
        "```nn.Linear()```\n",
        "\n",
        "This is a Rectified Linear Unit activation function. It applies the function f(x) = max(0,x) to every number on the output. This ultimately turns every negative value 0 and doesn't touch positive values\n",
        "\n",
        "This introduces non-linearity into the data and allows the model to learn complex relationships rather than linear ones\n",
        "--> why does this function do this?\n",
        "\n",
        "```nn.Linear(512,512)```\n",
        "\n",
        "This is another fully connected layer. Takes the 512 numbers from previous layer and outputs 512 numbers.\n",
        "\n",
        "This is considered a hidden layer. More layers = more capacity to learn. You're deepening the ability for network to learn complex patterns\n",
        "\n",
        "```nn.ReLU()```\n",
        "\n",
        "Same as before, adds non-linearity again\n",
        "--> why do we need this again\n",
        "\n",
        "```nn.Linear(512,10)```\n",
        "\n",
        "This is the output layer of the network. It takes the 512 values from previous layer and outputs 10 values, one for each class of the FashionMNIST data set. This is a tensor with shape [10]\n",
        "\n",
        "This output is called ```logits``` and is the raw score before turning it into a probability\n",
        "\n",
        "```result = tensor([2.4, -1.2, 0.3..., 1.1])```\n",
        "\n",
        "This is a 10-element tensor where each number is a score for each class. The highest one is the predicted label.\n",
        "\n",
        "## now lets work through the forward component of this\n",
        "\n",
        "```\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "```\n",
        "\n",
        "```def forward(self, x):```\n",
        "\n",
        "Every model need a forward method that tells you, when I send data through this model, this is what you should do.\n",
        "```(self,x)``` really means when someone calls model.forward(x), use this specific model (self) and the input tensor 'x'\n",
        "* X is usually the input tensor, in this case a batch of images\n",
        "* X will be shaped like [batch_size, 1, 28, 28]. This is the actual image data that you are going to pass through\n",
        "\n",
        "```self.flatten = nn.Flatten()```\n",
        "\n",
        "This takes a 2D image (shape=(batch_size, 1, 28, 28) into a 1D vector that is (batch_size, 784)\n",
        "* Neural networks expect flat vectors, not 2D vectors. So you reshape the image to a flat line\n",
        "\n",
        "```logits = self.linear_relu_stack(x)```\n",
        "\n",
        "You're passing the flattened input ```x``` through the layer stack that we created earlier\n",
        "\n",
        "It runs the layer through each step that we outlined above. It transforms the input into a higher-level feature and outputs 10 numbers (logits)\n",
        "\n",
        "```return logits```\n",
        "\n",
        "This returns the final result of the model: a tensor in the shape [batch_size, 10]. This output is used for training or making predictions. We'll typically pass this into a loss function and identify the predicted class label.\n",
        "\n",
        "## now lets examine the output\n",
        "\n",
        "```\n",
        "NeuralNetwork(\n",
        "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
        "  (linear_relu_stack): Sequential(\n",
        "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
        "    (1): ReLU()\n",
        "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
        "    (3): ReLU()\n",
        "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
        "  )\n",
        ")\n",
        "```\n",
        "\n",
        "```NeuralNetwork``` is the name out of the output class (e.g. this is a model of type Neural Network\n",
        "\n",
        "```(flatten): Flatten(start_dim=1, end_dim=-1)``` is the first layer of our model that flatten it starting at dimension 1.\n",
        "* This is because dim=0 is usually batch size\n",
        "* Dimension 1-3 are the image itself\n",
        "* so it flattens the image into a 1D vector\n",
        "\n",
        "```(linear_relu_stack): Sequential(...)```\n",
        "This is the stack of layers one by one. Bias = true means each output also contains a biar term\n"
      ],
      "metadata": {
        "id": "y8CJLrCALFOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimizing Model Parameters **"
      ],
      "metadata": {
        "id": "ODdJ7ANOzhWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## this is defining our loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "## this is the actual optimization function / training loop\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## compute the prediction error\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred,y)\n",
        "\n",
        "      ##backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss_val, current = loss.item(), (batch + 1) *len(X)\n",
        "        print(f\"loss: {loss_val:>7f} [{current:>5d}/{size:>5d}]\")\n"
      ],
      "metadata": {
        "id": "9u4VtG1OztlS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Block Explanation ##\n",
        "\n",
        "This is the full code block that we are working with. Lets go line by line through this.\n",
        "```\n",
        "## this is defining our loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "## this is the actual optimization function / training loop\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## compute the prediction error\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred,y)\n",
        "\n",
        "      ##backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), (batch + 1) *len(x)\n",
        "        print(f\"loss: {loss_val:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "```\n",
        "\n",
        "**Defining our loss function**\n",
        "```\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "```\n",
        "```loss_fn = nn.CrossEntropyLoss()``` this is also known as the objective function\n",
        "* Cross-Entropy Loss is the standard for multi-class classification problems.\n",
        "* This essentially compares the models predicted scores (logits) to the true labels and calculates how wrong the predictions are\n",
        "\n",
        "Here are some different types of loss functions for classifications and their common use cases:\n",
        "* nn.Cross-EntropyLoss() --> most common for multi-class classification problems. Combeins LogSoftmax and Negative Log Likelihood under the hood\n",
        "* nn.BCELoss() - Binary Cross-Entropy Loss --> most commonly use for binary classification. Works with sigmoid() at the output layer\n",
        "* nn.BCEwithLogitsLoss() - same as above, but includes the sigmoid step inside, more stable\n",
        "* nn.NLLLoss() -- Negative Log Likelihood -- used when your model ends with LogSoftMax, less common because CrossEntropyLoss handles this\n",
        "\n",
        "Here are some different loss functions for regression\n",
        "* nn.MSELoss() -- Mean Squared Error --> common tasks like predicting price or temp, measures the average of the squared differences between predictions and targets\n",
        "* nn.L1Loss() -- Mean Absolute Error --> Measures the avg of the absolute differences between predictions and targets. Less sensitive to outliers than MSE.\n",
        "* nn.SmothL1Loss() -- Huber Loss --> A hybrid between the MSE and L1\n",
        "\n",
        "**Defining our optimizer**\n",
        "\n",
        "```optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)```\n",
        "\n",
        "this is the \"coach\" to your model that helps it learn better by adjusting the weights (models parameters) based on how wrong its predictions are\n",
        "\n",
        "```torch.optim.SGD``` is a Stochastic Gradient Descent --> you're saying \"Use SGF to help me update my model's weights\n",
        "* This method udpates the models weights based on gradients that were calculated through backpropagation\n",
        "* Stochastic = random batches not the full data set which makes it faster\n",
        "\n",
        "```model.parameters()``` grabs all the weights and biases in your model that need to be trained ---> can we get a reminder on what parameters are in the original model\n",
        "* These are the numbers inside your layers\n",
        "* You're saying here are the knows you can turn\n",
        "\n",
        "Quick refresher on what parameters are. These are the info in your fully connected linear layers. Other layers don't have parameters.\n",
        "* weights = how much importance to give to each input\n",
        "* biases = startpoint or adjustment for the neuron\n",
        "* ```nn.Linear(784,512)``` - weights - matrix shaped: [512,784] & bias vector shaped [512]\n",
        "* in our function, we will be adjusting 3 weights, and 3 biases to make sure they are right\n",
        "\n",
        "```lr=1e-3``` - this is the learning rate of the model each time updates the weights. Scientific notation\n",
        "* Its a small number on purpose to prevent the model from taking too big of a wrong step\n",
        "--> what are other options for the learning rate?"
      ],
      "metadata": {
        "id": "dmRHAva31v4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define our training loop**\n",
        "\n",
        "```\n",
        "## this is the actual optimization function / training loop\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "```\n",
        "\n",
        "```def train(dataloader, model, loss_fn, optimizer):``` you are defining a function that is called \"train\"\n",
        "* dataloader = batch by batch provider of training data\n",
        "* model = neutral network you want to train\n",
        "* loss_fn = how we measure how wrong the model is\n",
        "* optimizer = how the model should imporve/adjust weights\n",
        "\n",
        "```size = len(dataloader.dataset)``` tells us how many total examples in the dataset so we can print progress\n",
        "\n",
        "```model.train()``` puts the model in training mode, which is important for layeers like dropout and batchnorm which treat inputs differently in training or testing\n",
        "\n",
        "```for batch, (X, y) in enumerate(dataloader):``` tells you to go through  dataset one batch at a time. Each batch gives you x group of images an y true labels for those images\n",
        "* enumerate adds an index to any iterable so you can keep track of what loop you are on\n",
        "\n",
        "```X, y = X.to(device), y.to(device)``` moves the batch of images to the right device. GPU if available or CPU if not."
      ],
      "metadata": {
        "id": "CzXksryuqZpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computer our Prediction Error**\n",
        "\n",
        "```\n",
        "   ## compute the prediction error\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred,y)\n",
        "```\n",
        "\n",
        "```pred = model(x)``` send the images to the model to get predictions. The output of this is the logits for each class\n",
        "\n",
        "```loss = loss_fn(pred,y)``` calculate the loss or how wrong the model is with the goal of reducing this number over time\n",
        "\n"
      ],
      "metadata": {
        "id": "gZWBM9-9v5FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation and Optimization**\n",
        "\n",
        "```\n",
        "##backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "```\n",
        "\n",
        "```loss.backward()``` -- look at the loss and figure out how to adjust the models internal weights --> this results in the gradients, the direct amounts to adjust each parameter\n",
        "\n",
        "```optimizer.step()``` -- This is the actual learning step. Use the gradients to update the weights.\n",
        "\n",
        "```optimizer.zero_grad()``` -- Clear out the old gradients from the previous round. If you dont do this the gradients will stack up"
      ],
      "metadata": {
        "id": "XpF9CzO3wdSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Progress**\n",
        "\n",
        "```\n",
        "if batch % 100 == 0:\n",
        "    loss, current = loss.item(), (batch + 1) * len(X)\n",
        "    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "```\n",
        "This only runs every 100 batches (% means remainder) and prints out the current loss and how many examples we've trained on\n",
        "* % gives you the remainder when dividing 2 numbers. By saying batch % 100 we're saying only run the print function when batch is divisible by 100\n",
        "* loss.item is a tensor that is converted to a python number\n",
        "* (batch+1) * len(x) is the current batch number * the number of examples in a batch which tells you how many total examples you've trained so far"
      ],
      "metadata": {
        "id": "eJM41aEfw_40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Performance Against Test Data Set to Ensure that its learning**"
      ],
      "metadata": {
        "id": "2PkiiKzm0R7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "      for X, y in dataloader:\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        test_loss += loss_fn(pred, y).item()\n",
        "        correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "9lkrnQZ20ZN6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "At a high level, this step runs the model on the test data and measures (1) how wrong the predictions are and (2) how many predictions were correct\n",
        "\n",
        "**Setup for Test Model**\n",
        "\n",
        "```def test(dataloader, model, loss_fn):``` this defines a function called test() and uses a dataloader for your test data, your model, and the same loss function you're using\n",
        "\n",
        "```  size = len(dataloader.dataset)``` gets the total number of test examples (in this case its 10k)\n",
        "\n",
        "```  num_batches = len(dataloader)``` gets the number of batches in the dataloader, which in this case is 157 if they each have 64. This is used to compute the avg test loss\n",
        "\n",
        "```model.eval()``` switch the model to eval mode which will impact how certain layers function by disabling them\n",
        "\n",
        "```test_loss, correct = 0, 0``` initializing 2 variables that tell us the test_loss across all batches and number of correct across all batches\n",
        "\n",
        "```with torch.no_grad():``` tells the model, dont track gradients or do backpropagation, we're just evaluating\n",
        "\n",
        "**Loop through the Test Data**\n",
        "\n",
        "\n",
        "```for X, y in dataloader:``` --> you're telling the model to use the batches of images and the labels for those images\n",
        "```pred= model(x)``` do the forward pass trhough your model to get the logits or raw predictions\n",
        "\n",
        "```test_loss += loss_fn(pred,y).item()```\n",
        "This calculates the loss for a current batch.\n",
        "* loss_fn gives you the lose tensor\n",
        "* item() converts it to a number\n",
        "* += adds it to running list for test_loss\n",
        "\n",
        "```correct += (pred.argmax(1) == y).type(torch.float).sum().item()```\n",
        "* pred.argmax(1) -- for each prediction, finds the index with the highest score. It finds the best class for each image\n",
        "* (pred.argmax(1) == y) --> compares the predicted class to the true class which will return a True/False value\n",
        "* .type(torch.float) converts the boolean to 0 or 1\n",
        "* .sum().item() --> adds up the correct predictions in the batch and converts the result to a number you add to correct\n",
        "\n",
        "```test_loss /= num_batches```\n",
        "We devide the test_loss for all of the batches by the number of batches to get the avg batch loss\n",
        "\n",
        "```correct /= size``` means we divide the number of correct predictions by total number of test examples\n"
      ],
      "metadata": {
        "id": "2vsSs0th1r9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqe5g0Rv1Z4U",
        "outputId": "d0534dba-27e2-421f-8e04-86c5c903a980"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302102 [   64/60000]\n",
            "loss: 2.288951 [ 6464/60000]\n",
            "loss: 2.271698 [12864/60000]\n",
            "loss: 2.268208 [19264/60000]\n",
            "loss: 2.249953 [25664/60000]\n",
            "loss: 2.230263 [32064/60000]\n",
            "loss: 2.228659 [38464/60000]\n",
            "loss: 2.202903 [44864/60000]\n",
            "loss: 2.195704 [51264/60000]\n",
            "loss: 2.164429 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 46.7%, Avg loss: 2.157645 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.167465 [   64/60000]\n",
            "loss: 2.149531 [ 6464/60000]\n",
            "loss: 2.101513 [12864/60000]\n",
            "loss: 2.121039 [19264/60000]\n",
            "loss: 2.058961 [25664/60000]\n",
            "loss: 2.011565 [32064/60000]\n",
            "loss: 2.034827 [38464/60000]\n",
            "loss: 1.960528 [44864/60000]\n",
            "loss: 1.967425 [51264/60000]\n",
            "loss: 1.889371 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 1.888002 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.923011 [   64/60000]\n",
            "loss: 1.880771 [ 6464/60000]\n",
            "loss: 1.779556 [12864/60000]\n",
            "loss: 1.824528 [19264/60000]\n",
            "loss: 1.701186 [25664/60000]\n",
            "loss: 1.666509 [32064/60000]\n",
            "loss: 1.687850 [38464/60000]\n",
            "loss: 1.595124 [44864/60000]\n",
            "loss: 1.622780 [51264/60000]\n",
            "loss: 1.514916 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 58.7%, Avg loss: 1.531135 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.598429 [   64/60000]\n",
            "loss: 1.555168 [ 6464/60000]\n",
            "loss: 1.421247 [12864/60000]\n",
            "loss: 1.491107 [19264/60000]\n",
            "loss: 1.366187 [25664/60000]\n",
            "loss: 1.375921 [32064/60000]\n",
            "loss: 1.384756 [38464/60000]\n",
            "loss: 1.315369 [44864/60000]\n",
            "loss: 1.348627 [51264/60000]\n",
            "loss: 1.248489 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.3%, Avg loss: 1.268633 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.344169 [   64/60000]\n",
            "loss: 1.319603 [ 6464/60000]\n",
            "loss: 1.166594 [12864/60000]\n",
            "loss: 1.269493 [19264/60000]\n",
            "loss: 1.142499 [25664/60000]\n",
            "loss: 1.179330 [32064/60000]\n",
            "loss: 1.193098 [38464/60000]\n",
            "loss: 1.136393 [44864/60000]\n",
            "loss: 1.174414 [51264/60000]\n",
            "loss: 1.085798 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 1.101457 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.170231 [   64/60000]\n",
            "loss: 1.166293 [ 6464/60000]\n",
            "loss: 0.995252 [12864/60000]\n",
            "loss: 1.128291 [19264/60000]\n",
            "loss: 1.000416 [25664/60000]\n",
            "loss: 1.044247 [32064/60000]\n",
            "loss: 1.071949 [38464/60000]\n",
            "loss: 1.019341 [44864/60000]\n",
            "loss: 1.059349 [51264/60000]\n",
            "loss: 0.980480 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.5%, Avg loss: 0.991651 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.047264 [   64/60000]\n",
            "loss: 1.064844 [ 6464/60000]\n",
            "loss: 0.876304 [12864/60000]\n",
            "loss: 1.033149 [19264/60000]\n",
            "loss: 0.909336 [25664/60000]\n",
            "loss: 0.947860 [32064/60000]\n",
            "loss: 0.991864 [38464/60000]\n",
            "loss: 0.941312 [44864/60000]\n",
            "loss: 0.978544 [51264/60000]\n",
            "loss: 0.908900 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 0.915976 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.955872 [   64/60000]\n",
            "loss: 0.993312 [ 6464/60000]\n",
            "loss: 0.790280 [12864/60000]\n",
            "loss: 0.965425 [19264/60000]\n",
            "loss: 0.848252 [25664/60000]\n",
            "loss: 0.876869 [32064/60000]\n",
            "loss: 0.935334 [38464/60000]\n",
            "loss: 0.888175 [44864/60000]\n",
            "loss: 0.919478 [51264/60000]\n",
            "loss: 0.857377 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.5%, Avg loss: 0.861281 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.885352 [   64/60000]\n",
            "loss: 0.939712 [ 6464/60000]\n",
            "loss: 0.725576 [12864/60000]\n",
            "loss: 0.914846 [19264/60000]\n",
            "loss: 0.804897 [25664/60000]\n",
            "loss: 0.823206 [32064/60000]\n",
            "loss: 0.892709 [38464/60000]\n",
            "loss: 0.850820 [44864/60000]\n",
            "loss: 0.874961 [51264/60000]\n",
            "loss: 0.818163 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.819866 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.829150 [   64/60000]\n",
            "loss: 0.897063 [ 6464/60000]\n",
            "loss: 0.675010 [12864/60000]\n",
            "loss: 0.875786 [19264/60000]\n",
            "loss: 0.772131 [25664/60000]\n",
            "loss: 0.781972 [32064/60000]\n",
            "loss: 0.858508 [38464/60000]\n",
            "loss: 0.823067 [44864/60000]\n",
            "loss: 0.840266 [51264/60000]\n",
            "loss: 0.786735 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.2%, Avg loss: 0.787082 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.782969 [   64/60000]\n",
            "loss: 0.861367 [ 6464/60000]\n",
            "loss: 0.634263 [12864/60000]\n",
            "loss: 0.844830 [19264/60000]\n",
            "loss: 0.746041 [25664/60000]\n",
            "loss: 0.749701 [32064/60000]\n",
            "loss: 0.829609 [38464/60000]\n",
            "loss: 0.801316 [44864/60000]\n",
            "loss: 0.812170 [51264/60000]\n",
            "loss: 0.760443 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.3%, Avg loss: 0.760042 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.744097 [   64/60000]\n",
            "loss: 0.830340 [ 6464/60000]\n",
            "loss: 0.600502 [12864/60000]\n",
            "loss: 0.819613 [19264/60000]\n",
            "loss: 0.724225 [25664/60000]\n",
            "loss: 0.723875 [32064/60000]\n",
            "loss: 0.804116 [38464/60000]\n",
            "loss: 0.783407 [44864/60000]\n",
            "loss: 0.788733 [51264/60000]\n",
            "loss: 0.737828 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.3%, Avg loss: 0.736944 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.710692 [   64/60000]\n",
            "loss: 0.802761 [ 6464/60000]\n",
            "loss: 0.571973 [12864/60000]\n",
            "loss: 0.798513 [19264/60000]\n",
            "loss: 0.705393 [25664/60000]\n",
            "loss: 0.702862 [32064/60000]\n",
            "loss: 0.780990 [38464/60000]\n",
            "loss: 0.767990 [44864/60000]\n",
            "loss: 0.768724 [51264/60000]\n",
            "loss: 0.717822 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.716689 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.681716 [   64/60000]\n",
            "loss: 0.777962 [ 6464/60000]\n",
            "loss: 0.547390 [12864/60000]\n",
            "loss: 0.780477 [19264/60000]\n",
            "loss: 0.688839 [25664/60000]\n",
            "loss: 0.685453 [32064/60000]\n",
            "loss: 0.759695 [38464/60000]\n",
            "loss: 0.754499 [44864/60000]\n",
            "loss: 0.751245 [51264/60000]\n",
            "loss: 0.699666 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.0%, Avg loss: 0.698574 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.656432 [   64/60000]\n",
            "loss: 0.755512 [ 6464/60000]\n",
            "loss: 0.525892 [12864/60000]\n",
            "loss: 0.764727 [19264/60000]\n",
            "loss: 0.674183 [25664/60000]\n",
            "loss: 0.670802 [32064/60000]\n",
            "loss: 0.740013 [38464/60000]\n",
            "loss: 0.742612 [44864/60000]\n",
            "loss: 0.735895 [51264/60000]\n",
            "loss: 0.683150 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.7%, Avg loss: 0.682226 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.634247 [   64/60000]\n",
            "loss: 0.734931 [ 6464/60000]\n",
            "loss: 0.507155 [12864/60000]\n",
            "loss: 0.750576 [19264/60000]\n",
            "loss: 0.661377 [25664/60000]\n",
            "loss: 0.658365 [32064/60000]\n",
            "loss: 0.721685 [38464/60000]\n",
            "loss: 0.732018 [44864/60000]\n",
            "loss: 0.722493 [51264/60000]\n",
            "loss: 0.667998 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.667372 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.614686 [   64/60000]\n",
            "loss: 0.716144 [ 6464/60000]\n",
            "loss: 0.490638 [12864/60000]\n",
            "loss: 0.737740 [19264/60000]\n",
            "loss: 0.649958 [25664/60000]\n",
            "loss: 0.647725 [32064/60000]\n",
            "loss: 0.704662 [38464/60000]\n",
            "loss: 0.722489 [44864/60000]\n",
            "loss: 0.710866 [51264/60000]\n",
            "loss: 0.653983 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.9%, Avg loss: 0.653806 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.597290 [   64/60000]\n",
            "loss: 0.699054 [ 6464/60000]\n",
            "loss: 0.475965 [12864/60000]\n",
            "loss: 0.726011 [19264/60000]\n",
            "loss: 0.639843 [25664/60000]\n",
            "loss: 0.638502 [32064/60000]\n",
            "loss: 0.688942 [38464/60000]\n",
            "loss: 0.714078 [44864/60000]\n",
            "loss: 0.700873 [51264/60000]\n",
            "loss: 0.641051 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.641378 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.581672 [   64/60000]\n",
            "loss: 0.683460 [ 6464/60000]\n",
            "loss: 0.462795 [12864/60000]\n",
            "loss: 0.715273 [19264/60000]\n",
            "loss: 0.630778 [25664/60000]\n",
            "loss: 0.630394 [32064/60000]\n",
            "loss: 0.674410 [38464/60000]\n",
            "loss: 0.706734 [44864/60000]\n",
            "loss: 0.692351 [51264/60000]\n",
            "loss: 0.629085 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.8%, Avg loss: 0.629991 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.567661 [   64/60000]\n",
            "loss: 0.669250 [ 6464/60000]\n",
            "loss: 0.450964 [12864/60000]\n",
            "loss: 0.705331 [19264/60000]\n",
            "loss: 0.622515 [25664/60000]\n",
            "loss: 0.623159 [32064/60000]\n",
            "loss: 0.660894 [38464/60000]\n",
            "loss: 0.700368 [44864/60000]\n",
            "loss: 0.685099 [51264/60000]\n",
            "loss: 0.617908 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.2%, Avg loss: 0.619549 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.555005 [   64/60000]\n",
            "loss: 0.656285 [ 6464/60000]\n",
            "loss: 0.440277 [12864/60000]\n",
            "loss: 0.696090 [19264/60000]\n",
            "loss: 0.614943 [25664/60000]\n",
            "loss: 0.616725 [32064/60000]\n",
            "loss: 0.648448 [38464/60000]\n",
            "loss: 0.694881 [44864/60000]\n",
            "loss: 0.678877 [51264/60000]\n",
            "loss: 0.607411 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.4%, Avg loss: 0.609946 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.543496 [   64/60000]\n",
            "loss: 0.644400 [ 6464/60000]\n",
            "loss: 0.430517 [12864/60000]\n",
            "loss: 0.687440 [19264/60000]\n",
            "loss: 0.607823 [25664/60000]\n",
            "loss: 0.610860 [32064/60000]\n",
            "loss: 0.636987 [38464/60000]\n",
            "loss: 0.690306 [44864/60000]\n",
            "loss: 0.673548 [51264/60000]\n",
            "loss: 0.597480 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.7%, Avg loss: 0.601087 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.532874 [   64/60000]\n",
            "loss: 0.633505 [ 6464/60000]\n",
            "loss: 0.421545 [12864/60000]\n",
            "loss: 0.679317 [19264/60000]\n",
            "loss: 0.601002 [25664/60000]\n",
            "loss: 0.605322 [32064/60000]\n",
            "loss: 0.626350 [38464/60000]\n",
            "loss: 0.686682 [44864/60000]\n",
            "loss: 0.669001 [51264/60000]\n",
            "loss: 0.587967 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.592887 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.522894 [   64/60000]\n",
            "loss: 0.623464 [ 6464/60000]\n",
            "loss: 0.413253 [12864/60000]\n",
            "loss: 0.671708 [19264/60000]\n",
            "loss: 0.594378 [25664/60000]\n",
            "loss: 0.600104 [32064/60000]\n",
            "loss: 0.616485 [38464/60000]\n",
            "loss: 0.683729 [44864/60000]\n",
            "loss: 0.665032 [51264/60000]\n",
            "loss: 0.578943 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.4%, Avg loss: 0.585325 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.513586 [   64/60000]\n",
            "loss: 0.614175 [ 6464/60000]\n",
            "loss: 0.405632 [12864/60000]\n",
            "loss: 0.664535 [19264/60000]\n",
            "loss: 0.588015 [25664/60000]\n",
            "loss: 0.595087 [32064/60000]\n",
            "loss: 0.607307 [38464/60000]\n",
            "loss: 0.681404 [44864/60000]\n",
            "loss: 0.661623 [51264/60000]\n",
            "loss: 0.570374 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.578343 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.504889 [   64/60000]\n",
            "loss: 0.605627 [ 6464/60000]\n",
            "loss: 0.398583 [12864/60000]\n",
            "loss: 0.657705 [19264/60000]\n",
            "loss: 0.581812 [25664/60000]\n",
            "loss: 0.590236 [32064/60000]\n",
            "loss: 0.598690 [38464/60000]\n",
            "loss: 0.679598 [44864/60000]\n",
            "loss: 0.658655 [51264/60000]\n",
            "loss: 0.562141 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.9%, Avg loss: 0.571868 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.496673 [   64/60000]\n",
            "loss: 0.597707 [ 6464/60000]\n",
            "loss: 0.392015 [12864/60000]\n",
            "loss: 0.651182 [19264/60000]\n",
            "loss: 0.575796 [25664/60000]\n",
            "loss: 0.585487 [32064/60000]\n",
            "loss: 0.590659 [38464/60000]\n",
            "loss: 0.678259 [44864/60000]\n",
            "loss: 0.655973 [51264/60000]\n",
            "loss: 0.554207 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.565841 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.488866 [   64/60000]\n",
            "loss: 0.590379 [ 6464/60000]\n",
            "loss: 0.385889 [12864/60000]\n",
            "loss: 0.644944 [19264/60000]\n",
            "loss: 0.569827 [25664/60000]\n",
            "loss: 0.580798 [32064/60000]\n",
            "loss: 0.583224 [38464/60000]\n",
            "loss: 0.677336 [44864/60000]\n",
            "loss: 0.653596 [51264/60000]\n",
            "loss: 0.546526 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.560214 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.481394 [   64/60000]\n",
            "loss: 0.583547 [ 6464/60000]\n",
            "loss: 0.380147 [12864/60000]\n",
            "loss: 0.639026 [19264/60000]\n",
            "loss: 0.563870 [25664/60000]\n",
            "loss: 0.576163 [32064/60000]\n",
            "loss: 0.576284 [38464/60000]\n",
            "loss: 0.676707 [44864/60000]\n",
            "loss: 0.651396 [51264/60000]\n",
            "loss: 0.539071 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.554949 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.474272 [   64/60000]\n",
            "loss: 0.577206 [ 6464/60000]\n",
            "loss: 0.374771 [12864/60000]\n",
            "loss: 0.633358 [19264/60000]\n",
            "loss: 0.557919 [25664/60000]\n",
            "loss: 0.571503 [32064/60000]\n",
            "loss: 0.569782 [38464/60000]\n",
            "loss: 0.676333 [44864/60000]\n",
            "loss: 0.649269 [51264/60000]\n",
            "loss: 0.531854 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.550018 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.467451 [   64/60000]\n",
            "loss: 0.571367 [ 6464/60000]\n",
            "loss: 0.369719 [12864/60000]\n",
            "loss: 0.627919 [19264/60000]\n",
            "loss: 0.552010 [25664/60000]\n",
            "loss: 0.566812 [32064/60000]\n",
            "loss: 0.563695 [38464/60000]\n",
            "loss: 0.676117 [44864/60000]\n",
            "loss: 0.647168 [51264/60000]\n",
            "loss: 0.524805 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.545390 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.460961 [   64/60000]\n",
            "loss: 0.565915 [ 6464/60000]\n",
            "loss: 0.364985 [12864/60000]\n",
            "loss: 0.622645 [19264/60000]\n",
            "loss: 0.546161 [25664/60000]\n",
            "loss: 0.562065 [32064/60000]\n",
            "loss: 0.557981 [38464/60000]\n",
            "loss: 0.676066 [44864/60000]\n",
            "loss: 0.645090 [51264/60000]\n",
            "loss: 0.517951 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.541041 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.454745 [   64/60000]\n",
            "loss: 0.560840 [ 6464/60000]\n",
            "loss: 0.360540 [12864/60000]\n",
            "loss: 0.617528 [19264/60000]\n",
            "loss: 0.540400 [25664/60000]\n",
            "loss: 0.557366 [32064/60000]\n",
            "loss: 0.552610 [38464/60000]\n",
            "loss: 0.676085 [44864/60000]\n",
            "loss: 0.643055 [51264/60000]\n",
            "loss: 0.511325 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.536948 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.448749 [   64/60000]\n",
            "loss: 0.556099 [ 6464/60000]\n",
            "loss: 0.356314 [12864/60000]\n",
            "loss: 0.612610 [19264/60000]\n",
            "loss: 0.534720 [25664/60000]\n",
            "loss: 0.552696 [32064/60000]\n",
            "loss: 0.547507 [38464/60000]\n",
            "loss: 0.676133 [44864/60000]\n",
            "loss: 0.641048 [51264/60000]\n",
            "loss: 0.504911 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.533085 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.442999 [   64/60000]\n",
            "loss: 0.551672 [ 6464/60000]\n",
            "loss: 0.352285 [12864/60000]\n",
            "loss: 0.607838 [19264/60000]\n",
            "loss: 0.529179 [25664/60000]\n",
            "loss: 0.548182 [32064/60000]\n",
            "loss: 0.542689 [38464/60000]\n",
            "loss: 0.676179 [44864/60000]\n",
            "loss: 0.639094 [51264/60000]\n",
            "loss: 0.498755 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.529428 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.437463 [   64/60000]\n",
            "loss: 0.547516 [ 6464/60000]\n",
            "loss: 0.348485 [12864/60000]\n",
            "loss: 0.603211 [19264/60000]\n",
            "loss: 0.523769 [25664/60000]\n",
            "loss: 0.543713 [32064/60000]\n",
            "loss: 0.538133 [38464/60000]\n",
            "loss: 0.676207 [44864/60000]\n",
            "loss: 0.637097 [51264/60000]\n",
            "loss: 0.492885 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.525961 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.432088 [   64/60000]\n",
            "loss: 0.543582 [ 6464/60000]\n",
            "loss: 0.344872 [12864/60000]\n",
            "loss: 0.598669 [19264/60000]\n",
            "loss: 0.518466 [25664/60000]\n",
            "loss: 0.539316 [32064/60000]\n",
            "loss: 0.533850 [38464/60000]\n",
            "loss: 0.676137 [44864/60000]\n",
            "loss: 0.635115 [51264/60000]\n",
            "loss: 0.487284 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.522676 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.426851 [   64/60000]\n",
            "loss: 0.539882 [ 6464/60000]\n",
            "loss: 0.341391 [12864/60000]\n",
            "loss: 0.594248 [19264/60000]\n",
            "loss: 0.513347 [25664/60000]\n",
            "loss: 0.535032 [32064/60000]\n",
            "loss: 0.529774 [38464/60000]\n",
            "loss: 0.676001 [44864/60000]\n",
            "loss: 0.633145 [51264/60000]\n",
            "loss: 0.481983 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.6%, Avg loss: 0.519559 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.421792 [   64/60000]\n",
            "loss: 0.536368 [ 6464/60000]\n",
            "loss: 0.338096 [12864/60000]\n",
            "loss: 0.589945 [19264/60000]\n",
            "loss: 0.508403 [25664/60000]\n",
            "loss: 0.530901 [32064/60000]\n",
            "loss: 0.525847 [38464/60000]\n",
            "loss: 0.675749 [44864/60000]\n",
            "loss: 0.631155 [51264/60000]\n",
            "loss: 0.476899 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.516594 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.416909 [   64/60000]\n",
            "loss: 0.533071 [ 6464/60000]\n",
            "loss: 0.334960 [12864/60000]\n",
            "loss: 0.585744 [19264/60000]\n",
            "loss: 0.503529 [25664/60000]\n",
            "loss: 0.526847 [32064/60000]\n",
            "loss: 0.522070 [38464/60000]\n",
            "loss: 0.675412 [44864/60000]\n",
            "loss: 0.629176 [51264/60000]\n",
            "loss: 0.471981 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.513776 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.412129 [   64/60000]\n",
            "loss: 0.529966 [ 6464/60000]\n",
            "loss: 0.331982 [12864/60000]\n",
            "loss: 0.581661 [19264/60000]\n",
            "loss: 0.498809 [25664/60000]\n",
            "loss: 0.522891 [32064/60000]\n",
            "loss: 0.518475 [38464/60000]\n",
            "loss: 0.674963 [44864/60000]\n",
            "loss: 0.627147 [51264/60000]\n",
            "loss: 0.467289 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.511088 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.407459 [   64/60000]\n",
            "loss: 0.527026 [ 6464/60000]\n",
            "loss: 0.329096 [12864/60000]\n",
            "loss: 0.577682 [19264/60000]\n",
            "loss: 0.494236 [25664/60000]\n",
            "loss: 0.519063 [32064/60000]\n",
            "loss: 0.515062 [38464/60000]\n",
            "loss: 0.674381 [44864/60000]\n",
            "loss: 0.625032 [51264/60000]\n",
            "loss: 0.462782 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.508526 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.402893 [   64/60000]\n",
            "loss: 0.524234 [ 6464/60000]\n",
            "loss: 0.326313 [12864/60000]\n",
            "loss: 0.573777 [19264/60000]\n",
            "loss: 0.489782 [25664/60000]\n",
            "loss: 0.515322 [32064/60000]\n",
            "loss: 0.511791 [38464/60000]\n",
            "loss: 0.673669 [44864/60000]\n",
            "loss: 0.622912 [51264/60000]\n",
            "loss: 0.458481 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.506075 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.398443 [   64/60000]\n",
            "loss: 0.521532 [ 6464/60000]\n",
            "loss: 0.323669 [12864/60000]\n",
            "loss: 0.569991 [19264/60000]\n",
            "loss: 0.485494 [25664/60000]\n",
            "loss: 0.511688 [32064/60000]\n",
            "loss: 0.508648 [38464/60000]\n",
            "loss: 0.672797 [44864/60000]\n",
            "loss: 0.620757 [51264/60000]\n",
            "loss: 0.454424 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.503721 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.394133 [   64/60000]\n",
            "loss: 0.518912 [ 6464/60000]\n",
            "loss: 0.321119 [12864/60000]\n",
            "loss: 0.566299 [19264/60000]\n",
            "loss: 0.481273 [25664/60000]\n",
            "loss: 0.508175 [32064/60000]\n",
            "loss: 0.505552 [38464/60000]\n",
            "loss: 0.671809 [44864/60000]\n",
            "loss: 0.618590 [51264/60000]\n",
            "loss: 0.450536 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.501459 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.389910 [   64/60000]\n",
            "loss: 0.516418 [ 6464/60000]\n",
            "loss: 0.318674 [12864/60000]\n",
            "loss: 0.562734 [19264/60000]\n",
            "loss: 0.477181 [25664/60000]\n",
            "loss: 0.504769 [32064/60000]\n",
            "loss: 0.502536 [38464/60000]\n",
            "loss: 0.670748 [44864/60000]\n",
            "loss: 0.616420 [51264/60000]\n",
            "loss: 0.446841 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.499288 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.385777 [   64/60000]\n",
            "loss: 0.514086 [ 6464/60000]\n",
            "loss: 0.316306 [12864/60000]\n",
            "loss: 0.559224 [19264/60000]\n",
            "loss: 0.473211 [25664/60000]\n",
            "loss: 0.501493 [32064/60000]\n",
            "loss: 0.499635 [38464/60000]\n",
            "loss: 0.669548 [44864/60000]\n",
            "loss: 0.614272 [51264/60000]\n",
            "loss: 0.443367 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.497201 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.381791 [   64/60000]\n",
            "loss: 0.511784 [ 6464/60000]\n",
            "loss: 0.314059 [12864/60000]\n",
            "loss: 0.555734 [19264/60000]\n",
            "loss: 0.469345 [25664/60000]\n",
            "loss: 0.498334 [32064/60000]\n",
            "loss: 0.496818 [38464/60000]\n",
            "loss: 0.668292 [44864/60000]\n",
            "loss: 0.612125 [51264/60000]\n",
            "loss: 0.440119 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.495197 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.377875 [   64/60000]\n",
            "loss: 0.509581 [ 6464/60000]\n",
            "loss: 0.311896 [12864/60000]\n",
            "loss: 0.552355 [19264/60000]\n",
            "loss: 0.465592 [25664/60000]\n",
            "loss: 0.495331 [32064/60000]\n",
            "loss: 0.494115 [38464/60000]\n",
            "loss: 0.666950 [44864/60000]\n",
            "loss: 0.610043 [51264/60000]\n",
            "loss: 0.437088 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.493259 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.374050 [   64/60000]\n",
            "loss: 0.507441 [ 6464/60000]\n",
            "loss: 0.309764 [12864/60000]\n",
            "loss: 0.549113 [19264/60000]\n",
            "loss: 0.462009 [25664/60000]\n",
            "loss: 0.492433 [32064/60000]\n",
            "loss: 0.491473 [38464/60000]\n",
            "loss: 0.665564 [44864/60000]\n",
            "loss: 0.607994 [51264/60000]\n",
            "loss: 0.434208 [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.7%, Avg loss: 0.491386 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20DwgKLr97DX",
        "outputId": "6aabe73e-99bf-413d-df5e-995323ad57dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzqmDzCv9_qx",
        "outputId": "7ba52d9d-7c5f-40f2-caaf-c150d9c5a931"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ETaNEcr-Cpa",
        "outputId": "914f1864-1d9b-43cf-e1ac-53564b52d7d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    }
  ]
}